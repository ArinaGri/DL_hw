{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Задача мультилейблинга в NLP**"
      ],
      "metadata": {
        "id": "sNC0EPY2M5Zc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim==4.3.2"
      ],
      "metadata": {
        "id": "x5nh-L70gFHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas==1.5.3"
      ],
      "metadata": {
        "id": "xgMQAm5Pd1g1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.24.4\n",
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "FdtZvwxPgji7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install protobuf==3.20.*"
      ],
      "metadata": {
        "id": "H1XLxYpNhO9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2"
      ],
      "metadata": {
        "id": "lndX-r4Yt0rU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras"
      ],
      "metadata": {
        "id": "woePbj5BviGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pymorphy2\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from nltk.stem.snowball import RussianStemmer\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LambdaCallback\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score, hamming_loss"
      ],
      "metadata": {
        "id": "UrzPUVJjDRq0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "GKNPAZPNUzQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "qqC2xU3sgBB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ADOmxsrNDwsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_train = '/content/drive/MyDrive/DL/comp2/train.csv'\n",
        "path_test = '/content/drive/MyDrive/DL/comp2/test.csv'"
      ],
      "metadata": {
        "id": "59JuMqj9gc9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(path_train)\n",
        "test = pd.read_csv(path_test)"
      ],
      "metadata": {
        "id": "IkAz9VocECTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1) Проанализировать данные, посмотреть на баланс классов, посмотреть на представителей классов, поизучать текста, сделать выводы. (0.5 балла).**"
      ],
      "metadata": {
        "id": "opmGxlqvNG14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "kmXOKA-YEJWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Всего записей: {len(df)}\")"
      ],
      "metadata": {
        "id": "w-_JCtEhEE1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_text = ' '.join(df['text'])\n",
        "word_freq = Counter(all_text.split())\n",
        "most_common = dict(word_freq.most_common(100))\n",
        "least_common = dict(word_freq.most_common()[:-101:-1])"
      ],
      "metadata": {
        "id": "-cm2r7TdfIyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_wordcloud(word_freq, title):\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ljbP5okYiAbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_wordcloud(most_common, \"Самые популярные слова\")"
      ],
      "metadata": {
        "id": "n3YvfYMjiEeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Из облака слов видно, что все тексты скорее всего как-то связаны с продажей билетов на разные культурные мероприятия. Так же видно, что очень часто встречаются предлоги, возможно стоит их убрать."
      ],
      "metadata": {
        "id": "8ORHAYwFN26q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[['text', 'labels']].sample(5))"
      ],
      "metadata": {
        "id": "KSVvCPywiM1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Много непонятных разделителей \\n, капслок, английский и смайлики"
      ],
      "metadata": {
        "id": "RmrY7CIqg5V8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_wordcloud(least_common, \"Некоторые наименее популярные слова\")"
      ],
      "metadata": {
        "id": "0IFvQa8xET4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Разделяю лейблы:"
      ],
      "metadata": {
        "id": "Kwr260YfNpvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = df['labels'].str.split(' ', expand=True)\n",
        "labels = labels.astype(int)\n",
        "num_classes = labels.shape[1]\n",
        "labels.columns = [f'label_{i}' for i in range(num_classes)]"
      ],
      "metadata": {
        "id": "sUYOSLG3r9DT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.concat([df, labels], axis=1)"
      ],
      "metadata": {
        "id": "PW7-DQ2Zo9J_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head(3)"
      ],
      "metadata": {
        "id": "X9XzWNFRpAf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_text = ' '.join(df_train[df_train['label_1']==1]['text'])\n",
        "word_freq = Counter(all_text.split())\n",
        "most_common = dict(word_freq.most_common(100))\n",
        "least_common = dict(word_freq.most_common()[:-101:-1])\n",
        "plot_wordcloud(most_common, \"Самые популярные слова в первом лейбле\")"
      ],
      "metadata": {
        "id": "lSQ0nxlhrrc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Тексты, относящиеся к разным классам, содержат в большинстве предлоги и такие слова, как \"билеты\", \"промокод\", \"скидка\" и \"ссылка\""
      ],
      "metadata": {
        "id": "WYXtRLXRPJQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_counts = labels.sum(axis=0)\n",
        "plt.figure(figsize=(15, 6))\n",
        "label_counts.plot(kind='bar')\n",
        "plt.title('Распределение меток классов')\n",
        "plt.xlabel('Классы')\n",
        "plt.ylabel('Количество')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lD-Q3aRTsvUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Здесь отчетливо виден дисбаланс классов. 2, 3, 5, 6 наименее репрезентативны. Скорее всего эти лейблы очень специфичны и возможно получиться выяснить, что это за классы\n",
        "\n",
        "К лейблам 7, 8, 12, 13, 14, 16, 17, 18 наоборот относятся почти все тексты. Есть смысл посмотреть на те, которые не относятся"
      ],
      "metadata": {
        "id": "cxw_s-IyPyHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_counts = labels.sum(axis=0)\n",
        "print(class_counts)"
      ],
      "metadata": {
        "id": "ep7kQWf5ySjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phone_pattern = r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]'\n",
        "phone_mask = df_train['text'].str.contains(phone_pattern, regex=True, na=False)\n",
        "texts_with_phones = df_train[phone_mask]"
      ],
      "metadata": {
        "id": "Cgrq9oCwWlib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_with_phones"
      ],
      "metadata": {
        "id": "BJ7NrJTiXtiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_with_phones.iloc[:, 3:24].sum()"
      ],
      "metadata": {
        "id": "MeG9k223WrYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2) Проанализировать, какие очистки текста необходимы под разные способы**\n",
        "\n",
        "1.   tf-idf + любая нейронная сеть;\n",
        "2.   w2v + любая нейронная сеть;\n",
        "3.   встроенный эмбеддинг в нейросетевое решение на свёрточных сетях;\n",
        "4.   встроенный эмбеддинг в нейросетевое решение на рекуррентных сетях;\n",
        "5.   эмбеддер + решение на bert-like моделях (любой вид).\n",
        "\n",
        " **Создать пайплайны очистки текста и очистить (1.5 балла).**\n",
        "\n",
        "## **3) Поделить данные на трейн-валидацию, обучить все модели из п.2. Снабдить обучение моделей графиками отрисовки лосей и метрик, шедулерами, свитч лосей (метрик), сохранение лучшей модели, ранней остановкой, вормапом. (5 баллов).**"
      ],
      "metadata": {
        "id": "jDiU2MS0YRuH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Визуализация метрик***"
      ],
      "metadata": {
        "id": "vFR9qcg1UfH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MetricsPlotter:\n",
        "    def __init__(self):\n",
        "        self.fig, (self.ax1, self.ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    def plot(self, history, model_name):\n",
        "        self.ax1.clear()\n",
        "        self.ax2.clear()\n",
        "\n",
        "        # График потерь\n",
        "        self.ax1.plot(history.history['loss'], label='Train Loss')\n",
        "        self.ax1.plot(history.history['val_loss'], label='Validation Loss')\n",
        "        self.ax1.set_title(f'{model_name} - Loss')\n",
        "        self.ax1.legend()\n",
        "\n",
        "        # График метрик\n",
        "        for metric in ['f1_score', 'accuracy']:\n",
        "            if metric in history.history:\n",
        "                self.ax2.plot(history.history[metric], label=f'Train {metric}')\n",
        "                self.ax2.plot(history.history[f'val_{metric}'], label=f'Val {metric}')\n",
        "        self.ax2.set_title(f'{model_name} - Metrics')\n",
        "        self.ax2.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.pause(0.1)"
      ],
      "metadata": {
        "id": "CT_IMQYfUaKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Шедулеры обучения***"
      ],
      "metadata": {
        "id": "jKdb0ozdU3uX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lr_scheduler(epoch, lr):\n",
        "    if epoch < 5:\n",
        "        return lr  # Warmup\n",
        "    return lr * tf.math.exp(-0.1)\n",
        "\n",
        "class CustomSchedule:\n",
        "    def __init__(self, warmup_steps=4000):\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "        return tf.math.rsqrt(64) * tf.math.minimum(arg1, arg2)"
      ],
      "metadata": {
        "id": "XTxairX5UmbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Переключение лоссов/метрик***"
      ],
      "metadata": {
        "id": "RaPFiKeoU9kA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MetricSwitcher(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, switch_epoch, new_metric):\n",
        "        super().__init__()\n",
        "        self.switch_epoch = switch_epoch\n",
        "        self.new_metric = new_metric\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if epoch == self.switch_epoch:\n",
        "            print(f\"\\nSwitching primary metric to {self.new_metric}\")\n",
        "            self.model.compile(\n",
        "                optimizer=self.model.optimizer,\n",
        "                loss=self.model.loss,\n",
        "                metrics=[self.new_metric]\n",
        "            )"
      ],
      "metadata": {
        "id": "gsWt3RKfU5yD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Ранняя остановка и сохранение***"
      ],
      "metadata": {
        "id": "CW8jycYLVI4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_callbacks(model_name):\n",
        "    return [\n",
        "        EarlyStopping(monitor='val_f1_score', patience=5, mode='max', verbose=1),\n",
        "        ModelCheckpoint(\n",
        "            f'best_{model_name}.h5',\n",
        "            monitor='val_f1_score',\n",
        "            save_best_only=True,\n",
        "            mode='max'\n",
        "        ),\n",
        "        LearningRateScheduler(lr_scheduler),\n",
        "        MetricSwitcher(switch_epoch=10)\n",
        "    ]"
      ],
      "metadata": {
        "id": "-n0eSyBqVF0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Warmup***"
      ],
      "metadata": {
        "id": "m3eztTIwVQEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WarmupCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, warmup_epochs=3):\n",
        "        super().__init__()\n",
        "        self.warmup_epochs = warmup_epochs\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        if epoch < self.warmup_epochs:\n",
        "            for layer in self.model.layers:\n",
        "                if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
        "                    layer.trainable = False\n",
        "        else:\n",
        "            for layer in self.model.layers:\n",
        "                layer.trainable = True"
      ],
      "metadata": {
        "id": "_KbuSsrOVMah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Очистка текста***"
      ],
      "metadata": {
        "id": "3KSKSNN6W_Bl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Технический код чтобы работал морф\n",
        "import inspect\n",
        "if not hasattr(inspect, 'getargspec'):\n",
        "    import collections\n",
        "    def getargspec(func):\n",
        "        sig = inspect.signature(func)\n",
        "        args = []\n",
        "        varargs = None\n",
        "        varkw = None\n",
        "        defaults = []\n",
        "        for param in sig.parameters.values():\n",
        "            if param.kind == param.VAR_POSITIONAL:\n",
        "                varargs = param.name\n",
        "            elif param.kind == param.VAR_KEYWORD:\n",
        "                varkw = param.name\n",
        "            else:\n",
        "                args.append(param.name)\n",
        "                if param.default is not param.empty:\n",
        "                    defaults.append(param.default)\n",
        "        return collections.namedtuple('ArgSpec', 'args varargs keywords defaults')(args, varargs, varkw, tuple(defaults) if defaults else None)\n",
        "    inspect.getargspec = getargspec"
      ],
      "metadata": {
        "id": "nWsk8Udwuikh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "morph = pymorphy2.MorphAnalyzer()\n",
        "stemmer = RussianStemmer()\n",
        "nltk.download('stopwords')\n",
        "russian_stopwords = set(stopwords.words('russian'))"
      ],
      "metadata": {
        "id": "qbggx99it-8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1) tf-idf + любая нейронная сеть**"
      ],
      "metadata": {
        "id": "43oFXAnEtldW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "nVXJcimm3ALy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text_tfidf(text):\n",
        "    # Удаление URL\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Удаление пунктуации\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Приведение к нижнему регистру\n",
        "    text = text.lower()\n",
        "    # Удаление чисел\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Токенизация\n",
        "    tokens = word_tokenize(text, language='russian')\n",
        "    # Лемматизация и удаление стоп-слов\n",
        "    tokens = [morph.parse(token)[0].normal_form for token in tokens if token not in russian_stopwords and len(token) > 2]\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "_vXC5SF41-zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2) w2v + любая нейронная сеть**"
      ],
      "metadata": {
        "id": "vZBdmVLCxJ1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text_w2v(text):\n",
        "    # Удаление URL\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Удаление пунктуации (кроме некоторых знаков)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation.replace('.', '').replace('!', '').replace('?', '')))\n",
        "    # Приведение к нижнему регистру\n",
        "    text = text.lower()\n",
        "    # Замена чисел на токен\n",
        "    text = re.sub(r'\\d+', '<NUM>', text)\n",
        "    # Токенизация\n",
        "    tokens = word_tokenize(text, language='russian')\n",
        "    # Удаление коротких токенов\n",
        "    tokens = [token for token in tokens if len(token) > 1]\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "p_HcNiGHxiuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3) встроенный эмбеддинг в нейросетевое решение на свёрточных сетях**"
      ],
      "metadata": {
        "id": "NY96600KxQkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text_cnn(text):\n",
        "    # Удаление URL (замена на токен)\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '<URL>', text, flags=re.MULTILINE)\n",
        "    # Удаление HTML тегов\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "    # Удаление пунктуации\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Приведение к нижнему регистру\n",
        "    text = text.lower()\n",
        "    # Замена эмодзи\n",
        "    text = re.sub(r'[\\U0001F600-\\U0001F64F]', '<EMOJI>', text)\n",
        "    # Замена чисел\n",
        "    text = re.sub(r'\\d+', '<NUM>', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "Kb107RvuxjHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4) встроенный эмбеддинг в нейросетевое решение на рекуррентных сетях**"
      ],
      "metadata": {
        "id": "mCn9T8GvxU3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text_rnn(text):\n",
        "    # Удаление URL (замена на токен)\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '<URL>', text, flags=re.MULTILINE)\n",
        "    # Удаление HTML тегов\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "    # Сохранение некоторых знаков препинания\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation.replace('.', '').replace('!', '').replace('?', '').replace(',', '')))\n",
        "    # Приведение к нижнему регистру\n",
        "    text = text.lower()\n",
        "    # Замена эмодзи\n",
        "    text = re.sub(r'[\\U0001F600-\\U0001F64F]', '<EMOJI>', text)\n",
        "    # Замена чисел\n",
        "    text = re.sub(r'\\d+', '<NUM>', text)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "mTq-zVTRxjb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5) эмбеддер + решение на bert-like моделях (любой вид)**"
      ],
      "metadata": {
        "id": "PjSUEuVAxdP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text_bert(text):\n",
        "    # Минимальная очистка для BERT\n",
        "    # Удаление лишних пробелов\n",
        "    text = ' '.join(text.split())\n",
        "    return text"
      ],
      "metadata": {
        "id": "FOAszhlsxjwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Применение очистки\n",
        "df_train['text_tfidf'] = df_train['text'].apply(clean_text_tfidf)\n",
        "df_train['text_w2v'] = df_train['text'].apply(clean_text_w2v)\n",
        "df_train['text_cnn'] = df_train['text'].apply(clean_text_cnn)\n",
        "df_train['text_rnn'] = df_train['text'].apply(clean_text_rnn)\n",
        "df_train['text_bert'] = df_train['text'].apply(clean_text_bert)"
      ],
      "metadata": {
        "id": "C7qBCNdQ2n0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_tfidf= test['text'].apply(clean_text_tfidf)\n",
        "X_test_w2v= test['text'].apply(clean_text_w2v)\n",
        "X_test_cnn= test['text'].apply(clean_text_cnn)\n",
        "X_test_rnn= test['text'].apply(clean_text_rnn)\n",
        "X_test_bert= test['text'].apply(clean_text_bert)\n",
        "test_ids = test['id']"
      ],
      "metadata": {
        "id": "KkzUVeQBHCRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_w2v"
      ],
      "metadata": {
        "id": "4cohLS4oK9R2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Поделить данные на трейн-валидацию**"
      ],
      "metadata": {
        "id": "1q0tt3lqZhri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Разделение на признаки и метки\n",
        "labels = df_train.filter(regex='label_').values\n",
        "texts_tfidf = df_train['text_tfidf'].values\n",
        "texts_w2v = df_train['text_w2v'].values\n",
        "texts_cnn = df_train['text_cnn'].values\n",
        "texts_rnn = df_train['text_rnn'].values\n",
        "texts_bert = df_train['text_bert'].values\n",
        "\n",
        "# Разделение на train/validation\n",
        "(X_train_tfidf, X_val_tfidf, y_train, y_val) = train_test_split(texts_tfidf, labels, test_size=0.2, random_state=42)\n",
        "(X_train_w2v, X_val_w2v, _, _) = train_test_split(texts_w2v, labels, test_size=0.2, random_state=42)\n",
        "(X_train_cnn, X_val_cnn, _, _) = train_test_split(texts_cnn, labels, test_size=0.2, random_state=42)\n",
        "(X_train_rnn, X_val_rnn, _, _) = train_test_split(texts_rnn, labels, test_size=0.2, random_state=42)\n",
        "(X_train_bert, X_val_bert, _, _) = train_test_split(texts_bert, labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Gnfg7QKx4V0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_w2v = df_train['text_w2v'].values\n",
        "texts_w2v"
      ],
      "metadata": {
        "id": "GrwLzKyzNfV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. tf-idf + любая нейронная сеть (скор на кагле)"
      ],
      "metadata": {
        "id": "CRQxl8B5KFvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = tfidf.fit_transform(X_train_tfidf).toarray()\n",
        "X_val_tfidf = tfidf.transform(X_val_tfidf).toarray()"
      ],
      "metadata": {
        "id": "jvMlQzyn5sve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " model = Sequential([\n",
        "     Dense(512, activation='relu', input_shape=(X_train_tfidf.shape[1],)),\n",
        "     Dropout(0.5),\n",
        "     Dense(256, activation='relu'),\n",
        "     Dropout(0.5),\n",
        "     Dense(y_train.shape[1], activation='sigmoid')\n",
        " ])\n",
        "\n",
        "\n",
        " model.compile(\n",
        "     optimizer=Adam(learning_rate=0.001),\n",
        "     loss='binary_crossentropy',\n",
        "     metrics=['accuracy']\n",
        " )\n",
        "\n",
        " history = model.fit(\n",
        "     X_train_tfidf, y_train,\n",
        "     validation_data=(X_val_tfidf, y_val),\n",
        "     epochs=10,\n",
        "     batch_size=32,\n",
        "     verbose=1\n",
        " )\n",
        "\n",
        "\n",
        " y_pred = model.predict(X_val_tfidf)\n",
        " y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "\n",
        " print(classification_report(y_val, y_pred_binary, target_names=[f'label_{i}' for i in range(y_train.shape[1])]))"
      ],
      "metadata": {
        "id": "o5Rs-9S06Eyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_tfidf = tfidf.transform(X_test_tfidf).toarray()\n",
        "\n",
        "\n",
        "y_test_pred = model.predict(X_test_tfidf)\n",
        "y_test_pred_binary = (y_test_pred > 0.5).astype(int)\n",
        "\n",
        "\n",
        "labels = [' '.join(map(str, row)) for row in y_test_pred_binary]\n",
        "\n",
        "\n",
        "results = pd.DataFrame({\n",
        "     'id': test_ids,\n",
        "     'labels': labels\n",
        " }).sort_values('id')"
      ],
      "metadata": {
        "id": "KOOejWmbHsn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ss = pd.read_csv('/content/drive/MyDrive/DL/comp2/sample_submission.csv')\n",
        "labels_as_string =[]\n",
        "for row in y_test_pred_binary:\n",
        "  str_row = [str(label) for label in row]\n",
        "  labels_as_string.append(' '.join(str_row))\n",
        "ss['labels'] = labels_as_string\n",
        "ss.to_csv('sample_submission.csv', index = False)"
      ],
      "metadata": {
        "id": "COmRHWjyH24m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ss"
      ],
      "metadata": {
        "id": "iMtpAFkHICAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  w2v + любая нейронная сеть"
      ],
      "metadata": {
        "id": "hgc3frUZGThK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, Flatten, Conv1D, GlobalMaxPooling1D,LSTM"
      ],
      "metadata": {
        "id": "sL4D78aXIuHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Обучение Word2Vec модели\n",
        "tokenized_texts = [text.split() for text in X_train]\n",
        "w2v_model = Word2Vec(tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Создание матрицы эмбеддингов\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in w2v_model.wv:\n",
        "        embedding_matrix[i] = w2v_model.wv[word]\n",
        "\n",
        "# Преобразование текстов в последовательности\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train_w2v)\n",
        "X_val_seq = tokenizer.texts_to_sequences(X_val_w2v)\n",
        "X_test_seq = tokenizer.texts_to_sequences(test['text'])\n",
        "\n",
        "max_len = 100\n",
        "X_train_w2v = pad_sequences(X_train_seq, maxlen=max_len)\n",
        "X_val_w2v = pad_sequences(X_val_seq, maxlen=max_len)\n",
        "X_test_w2v = pad_sequences(X_test_seq, maxlen=max_len)\n",
        "\n",
        "# Модель\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_len, trainable=False),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(y_train.shape[1], activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Обучение\n",
        "history = model.fit(\n",
        "    X_train_w2v, y_train,\n",
        "    validation_data=(X_val_w2v, y_val),\n",
        "    epochs=10,\n",
        "    batch_size=128,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Оценка\n",
        "y_pred = model.predict(X_val_w2v)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "print(\"Word2Vec + NN Classification Report:\")\n",
        "print(classification_report(y_val, y_pred_binary, target_names=[f'label_{i}' for i in range(y_train.shape[1])]))"
      ],
      "metadata": {
        "id": "X6RBd38RGWP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_pred = model.predict(X_test_w2v)\n",
        "y_test_pred_binary = (y_test_pred > 0.5).astype(int)\n",
        "\n",
        "\n",
        "labels = [' '.join(map(str, row)) for row in y_test_pred_binary]\n",
        "\n",
        "\n",
        "results = pd.DataFrame({\n",
        "     'id': test_ids,\n",
        "     'labels': labels\n",
        " }).sort_values('id')"
      ],
      "metadata": {
        "id": "4Bh_F2m8L0RY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ss = pd.read_csv('/content/drive/MyDrive/DL/comp2/sample_submission.csv')\n",
        "labels_as_string =[]\n",
        "for row in y_test_pred_binary:\n",
        "  str_row = [str(label) for label in row]\n",
        "  labels_as_string.append(' '.join(str_row))\n",
        "ss['labels'] = labels_as_string\n",
        "ss.to_csv('sample_submission.csv', index = False)"
      ],
      "metadata": {
        "id": "Vebya9TMMJo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3."
      ],
      "metadata": {
        "id": "vGqnog6nSVlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "max_len = 100\n",
        "\n",
        "# 2. Преобразование текстов в последовательности\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
        "X_test_seq = tokenizer.texts_to_sequences(test['text'])\n",
        "\n",
        "# 3. Добавление паддинга\n",
        "X_train_cnn = pad_sequences(X_train_seq, maxlen=max_len)\n",
        "X_val_cnn = pad_sequences(X_val_seq, maxlen=max_len)\n",
        "X_test_cnn = pad_sequences(X_test_seq, maxlen=max_len)\n",
        "\n",
        "# 4. Создание модели CNN\n",
        "model = Sequential([\n",
        "    Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=100,\n",
        "        input_length=max_len\n",
        "    ),\n",
        "    Conv1D(128, 5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(y_train.shape[1], activation='sigmoid')  # y_train - one-hot encoded labels\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 5. Обучение модели\n",
        "history = model.fit(\n",
        "    X_train_cnn, y_train,\n",
        "    validation_data=(X_val_cnn, y_val),\n",
        "    epochs=10,\n",
        "    batch_size=128,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 6. Оценка модели\n",
        "y_pred = model.predict(X_val_cnn)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "print(\"CNN Classification Report:\")\n",
        "print(classification_report(y_val, y_pred_binary, target_names=[f'label_{i}' for i in range(y_train.shape[1])]))"
      ],
      "metadata": {
        "id": "yW773PSySUxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4."
      ],
      "metadata": {
        "id": "W9rvopXs1Xhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_len, trainable=False),\n",
        "    LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(y_train.shape[1], activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_w2v, y_train,\n",
        "    validation_data=(X_val_w2v, y_val),\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "y_pred = model.predict(X_val_w2v)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "print(\"RNN (LSTM) Classification Report:\")\n",
        "print(classification_report(y_val, y_pred_binary, target_names=[f'label_{i}' for i in range(y_train.shape[1])]))"
      ],
      "metadata": {
        "id": "2NzJy31K166s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5."
      ],
      "metadata": {
        "id": "s3okhL_h1Oc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, TFBertModel\n",
        "import tensorflow as tf\n",
        "\n",
        "# Загрузка BERT модели и токенизатора\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "# Токенизация\n",
        "X_train_bert = tokenizer(X_train.tolist(), padding=True, truncation=True, max_length=100, return_tensors=\"tf\")\n",
        "X_val_bert = tokenizer(X_val.tolist(), padding=True, truncation=True, max_length=100, return_tensors=\"tf\")\n",
        "X_test_bert = tokenizer(X_test.tolist(), padding=True, truncation=True, max_length=100, return_tensors=\"tf\")\n",
        "\n",
        "# Создание модели\n",
        "input_ids = tf.keras.layers.Input(shape=(100,), dtype=tf.int32, name=\"input_ids\")\n",
        "attention_mask = tf.keras.layers.Input(shape=(100,), dtype=tf.int32, name=\"attention_mask\")\n",
        "\n",
        "bert_output = bert_model(input_ids, attention_mask=attention_mask)[1]\n",
        "dense1 = tf.keras.layers.Dense(256, activation='relu')(bert_output)\n",
        "dropout = tf.keras.layers.Dropout(0.5)(dense1)\n",
        "output = tf.keras.layers.Dense(y_train.shape[1], activation='sigmoid')(dropout)\n",
        "\n",
        "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=2e-5),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Обучение\n",
        "history = model.fit(\n",
        "    {'input_ids': X_train_bert['input_ids'], 'attention_mask': X_train_bert['attention_mask']},\n",
        "    y_train,\n",
        "    validation_data=({'input_ids': X_val_bert['input_ids'], 'attention_mask': X_val_bert['attention_mask']}, y_val),\n",
        "    epochs=3,  # BERT требует меньше эпох\n",
        "    batch_size=16,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Оценка\n",
        "y_pred = model.predict({'input_ids': X_val_bert['input_ids'], 'attention_mask': X_val_bert['attention_mask']})\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "print(\"BERT Classification Report:\")\n",
        "print(classification_report(y_val, y_pred_binary, target_names=[f'label_{i}' for i in range(y_train.shape[1])]))"
      ],
      "metadata": {
        "id": "ygEWWohW19FS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_test_clean = test['text'].apply(basic_clean)\n",
        "# test_ids = test['id']\n",
        "\n",
        "# X_test_tfidf = tfidf.transform(X_test_clean).toarray()\n",
        "\n",
        "\n",
        "# y_test_pred = model.predict(X_test_tfidf)\n",
        "# y_test_pred_binary = (y_test_pred > 0.5).astype(int)\n",
        "\n",
        "\n",
        "# labels = [' '.join(map(str, row)) for row in y_test_pred_binary]\n",
        "\n",
        "\n",
        "# results = pd.DataFrame({\n",
        "#     'id': test_ids,\n",
        "#     'labels': labels\n",
        "# }).sort_values('id')"
      ],
      "metadata": {
        "id": "-tt3Hi0UuawY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_test_clean = test['text'].apply(basic_clean)\n",
        "# test_ids = test['id']\n",
        "\n",
        "# X_test_tfidf = tfidf.transform(X_test_clean).toarray()\n",
        "\n",
        "\n",
        "# y_test_pred = model.predict(X_test_tfidf)\n",
        "# y_test_pred_binary = (y_test_pred > 0.5).astype(int)\n",
        "\n",
        "\n",
        "# labels = [' '.join(map(str, row)) for row in y_test_pred_binary]\n",
        "\n",
        "\n",
        "# results = pd.DataFrame({\n",
        "#     'id': test_ids,\n",
        "#     'labels': labels\n",
        "# }).sort_values('id')\n"
      ],
      "metadata": {
        "id": "rIxEzT2A6Jp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3x7dWhppEZTv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}