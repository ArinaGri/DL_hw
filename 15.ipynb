{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Задача мультилейблинга в NLP**"
      ],
      "metadata": {
        "id": "sNC0EPY2M5Zc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim==4.3.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5nh-L70gFHf",
        "outputId": "ab309d41-ebfc-4956-9579-a93c1de33c10"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim==4.3.2 in /usr/local/lib/python3.11/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3.2) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3.2) (1.15.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3.2) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim==4.3.2) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas==1.5.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "xgMQAm5Pd1g1",
        "outputId": "288d88b5-327f-4ed7-858b-8345d77def5c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas==1.5.3\n",
            "  Downloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3) (2025.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3) (2.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.17.0)\n",
            "Downloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "xarray 2025.3.1 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\n",
            "dask-expr 1.1.21 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-1.5.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              },
              "id": "b6b89572f8d3473eb49818901a61991e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.24.4\n",
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FdtZvwxPgji7",
        "outputId": "71b818ea-f85f-473e-f3d8-8e583a2a9018"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.24.4\n",
            "  Downloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Downloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "xarray 2025.3.1 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.4 which is incompatible.\n",
            "blosc2 3.3.2 requires numpy>=1.26, but you have numpy 1.24.4 which is incompatible.\n",
            "dask-expr 1.1.21 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.4 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.24.4 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.4 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "33ceb33712d5487f9cf76eda4cb80328"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Collecting numpy<2.1.0,>=1.26.0 (from tensorflow)\n",
            "  Downloading numpy-2.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading numpy-2.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.24.4\n",
            "    Uninstalling numpy-1.24.4:\n",
            "      Successfully uninstalled numpy-1.24.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "xarray 2025.3.1 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\n",
            "dask-expr 1.1.21 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install protobuf==3.20.*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "H1XLxYpNhO9_",
        "outputId": "d6fb2a16-cd78-4c4f-f16e-07ee22c56eb3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting protobuf==3.20.*\n",
            "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Downloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.17.1 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.20.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "591e9eddebac42e2aa92dcae5d885592"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lndX-r4Yt0rU",
        "outputId": "592e9904-cbbf-41f6-c8f8-35bb649fe44a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting dawg-python>=0.7.1 (from pymorphy2)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2)\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting docopt>=0.6 (from pymorphy2)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=029817a9542edd5b16a252677cda43c36291cb03ccad17b6f00247e576e45fcf\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "Successfully built docopt\n",
            "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woePbj5BviGo",
        "outputId": "9df9764c-fa18-4f56-d727-714c72a94a1c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikeras\n",
            "  Downloading scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from scikeras) (3.8.0)\n",
            "Requirement already satisfied: scikit-learn>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from scikeras) (1.6.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.0.9)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.15.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (24.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras>=3.2.0->scikeras) (4.13.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n",
            "Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pymorphy2\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from nltk.stem.snowball import RussianStemmer\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LambdaCallback\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score, hamming_loss"
      ],
      "metadata": {
        "id": "UrzPUVJjDRq0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "GKNPAZPNUzQe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "outputId": "22c97716-04b5-45a9-eb96-a3f60f3e93c9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'triu' from 'scipy.linalg' (/usr/local/lib/python3.11/dist-packages/scipy/linalg/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-384f1c26d32d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/corpora/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# bring corpus classes directly into package namespace, to save some typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mindexedcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexedCorpus\u001b[0m  \u001b[0;31m# noqa:F401 must appear before the other classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmmcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMmCorpus\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/corpora/indexedcorpus.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/interfaces.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/matutils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_blas_funcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtriu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlapack\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_lapack_funcs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpsi\u001b[0m  \u001b[0;31m# gamma function utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'triu' from 'scipy.linalg' (/usr/local/lib/python3.11/dist-packages/scipy/linalg/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "qqC2xU3sgBB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ADOmxsrNDwsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_train = '/content/drive/MyDrive/DL/comp2/train.csv'\n",
        "path_test = '/content/drive/MyDrive/DL/comp2/test.csv'"
      ],
      "metadata": {
        "id": "59JuMqj9gc9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(path_train)\n",
        "test = pd.read_csv(path_test)"
      ],
      "metadata": {
        "id": "IkAz9VocECTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1) Проанализировать данные, посмотреть на баланс классов, посмотреть на представителей классов, поизучать текста, сделать выводы. (0.5 балла).**"
      ],
      "metadata": {
        "id": "opmGxlqvNG14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "kmXOKA-YEJWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Всего записей: {len(df)}\")"
      ],
      "metadata": {
        "id": "w-_JCtEhEE1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_text = ' '.join(df['text'])\n",
        "word_freq = Counter(all_text.split())\n",
        "most_common = dict(word_freq.most_common(100))\n",
        "least_common = dict(word_freq.most_common()[:-101:-1])"
      ],
      "metadata": {
        "id": "-cm2r7TdfIyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_wordcloud(word_freq, title):\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ljbP5okYiAbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_wordcloud(most_common, \"Самые популярные слова\")"
      ],
      "metadata": {
        "id": "n3YvfYMjiEeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Из облака слов видно, что все тексты скорее всего как-то связаны с продажей билетов на разные культурные мероприятия. Так же видно, что очень часто встречаются предлоги, возможно стоит их убрать."
      ],
      "metadata": {
        "id": "8ORHAYwFN26q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[['text', 'labels']].sample(5))"
      ],
      "metadata": {
        "id": "KSVvCPywiM1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Много непонятных разделителей \\n, капслок, английский и смайлики"
      ],
      "metadata": {
        "id": "RmrY7CIqg5V8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_wordcloud(least_common, \"Некоторые наименее популярные слова\")"
      ],
      "metadata": {
        "id": "0IFvQa8xET4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Разделяю лейблы:"
      ],
      "metadata": {
        "id": "Kwr260YfNpvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = df['labels'].str.split(' ', expand=True)\n",
        "labels = labels.astype(int)\n",
        "num_classes = labels.shape[1]\n",
        "labels.columns = [f'label_{i}' for i in range(num_classes)]"
      ],
      "metadata": {
        "id": "sUYOSLG3r9DT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.concat([df, labels], axis=1)"
      ],
      "metadata": {
        "id": "PW7-DQ2Zo9J_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head(3)"
      ],
      "metadata": {
        "id": "X9XzWNFRpAf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_text = ' '.join(df_train[df_train['label_1']==1]['text'])\n",
        "word_freq = Counter(all_text.split())\n",
        "most_common = dict(word_freq.most_common(100))\n",
        "least_common = dict(word_freq.most_common()[:-101:-1])\n",
        "plot_wordcloud(most_common, \"Самые популярные слова в первом лейбле\")"
      ],
      "metadata": {
        "id": "lSQ0nxlhrrc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Тексты, относящиеся к разным классам, содержат в большинстве предлоги и такие слова, как \"билеты\", \"промокод\", \"скидка\" и \"ссылка\""
      ],
      "metadata": {
        "id": "WYXtRLXRPJQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_counts = labels.sum(axis=0)\n",
        "plt.figure(figsize=(15, 6))\n",
        "label_counts.plot(kind='bar')\n",
        "plt.title('Распределение меток классов')\n",
        "plt.xlabel('Классы')\n",
        "plt.ylabel('Количество')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lD-Q3aRTsvUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Здесь отчетливо виден дисбаланс классов. 2, 3, 5, 6 наименее репрезентативны. Скорее всего эти лейблы очень специфичны и возможно получиться выяснить, что это за классы\n",
        "\n",
        "К лейблам 7, 8, 12, 13, 14, 16, 17, 18 наоборот относятся почти все тексты. Есть смысл посмотреть на те, которые не относятся"
      ],
      "metadata": {
        "id": "cxw_s-IyPyHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_counts = labels.sum(axis=0)\n",
        "print(class_counts)"
      ],
      "metadata": {
        "id": "ep7kQWf5ySjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phone_pattern = r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]'\n",
        "phone_mask = df_train['text'].str.contains(phone_pattern, regex=True, na=False)\n",
        "texts_with_phones = df_train[phone_mask]"
      ],
      "metadata": {
        "id": "Cgrq9oCwWlib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_with_phones"
      ],
      "metadata": {
        "id": "BJ7NrJTiXtiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_with_phones.iloc[:, 3:24].sum()"
      ],
      "metadata": {
        "id": "MeG9k223WrYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2) Проанализировать, какие очистки текста необходимы под разные способы**\n",
        "\n",
        "1.   tf-idf + любая нейронная сеть;\n",
        "2.   w2v + любая нейронная сеть;\n",
        "3.   встроенный эмбеддинг в нейросетевое решение на свёрточных сетях;\n",
        "4.   встроенный эмбеддинг в нейросетевое решение на рекуррентных сетях;\n",
        "5.   эмбеддер + решение на bert-like моделях (любой вид).\n",
        "\n",
        " **Создать пайплайны очистки текста и очистить (1.5 балла).**\n",
        "\n",
        "## **3) Поделить данные на трейн-валидацию, обучить все модели из п.2. Снабдить обучение моделей графиками отрисовки лосей и метрик, шедулерами, свитч лосей (метрик), сохранение лучшей модели, ранней остановкой, вормапом. (5 баллов).**"
      ],
      "metadata": {
        "id": "jDiU2MS0YRuH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Визуализация метрик***"
      ],
      "metadata": {
        "id": "vFR9qcg1UfH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MetricsPlotter:\n",
        "    def __init__(self):\n",
        "        self.fig, (self.ax1, self.ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    def plot(self, history, model_name):\n",
        "        self.ax1.clear()\n",
        "        self.ax2.clear()\n",
        "\n",
        "        # График потерь\n",
        "        self.ax1.plot(history.history['loss'], label='Train Loss')\n",
        "        self.ax1.plot(history.history['val_loss'], label='Validation Loss')\n",
        "        self.ax1.set_title(f'{model_name} - Loss')\n",
        "        self.ax1.legend()\n",
        "\n",
        "        # График метрик\n",
        "        for metric in ['f1_score', 'accuracy']:\n",
        "            if metric in history.history:\n",
        "                self.ax2.plot(history.history[metric], label=f'Train {metric}')\n",
        "                self.ax2.plot(history.history[f'val_{metric}'], label=f'Val {metric}')\n",
        "        self.ax2.set_title(f'{model_name} - Metrics')\n",
        "        self.ax2.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.pause(0.1)"
      ],
      "metadata": {
        "id": "CT_IMQYfUaKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Шедулеры обучения***"
      ],
      "metadata": {
        "id": "jKdb0ozdU3uX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lr_scheduler(epoch, lr):\n",
        "    if epoch < 5:\n",
        "        return lr  # Warmup\n",
        "    return lr * tf.math.exp(-0.1)\n",
        "\n",
        "class CustomSchedule:\n",
        "    def __init__(self, warmup_steps=4000):\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "        return tf.math.rsqrt(64) * tf.math.minimum(arg1, arg2)"
      ],
      "metadata": {
        "id": "XTxairX5UmbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Переключение лоссов/метрик***"
      ],
      "metadata": {
        "id": "RaPFiKeoU9kA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MetricSwitcher(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, switch_epoch, new_metric):\n",
        "        super().__init__()\n",
        "        self.switch_epoch = switch_epoch\n",
        "        self.new_metric = new_metric\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if epoch == self.switch_epoch:\n",
        "            print(f\"\\nSwitching primary metric to {self.new_metric}\")\n",
        "            self.model.compile(\n",
        "                optimizer=self.model.optimizer,\n",
        "                loss=self.model.loss,\n",
        "                metrics=[self.new_metric]\n",
        "            )"
      ],
      "metadata": {
        "id": "gsWt3RKfU5yD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Ранняя остановка и сохранение***"
      ],
      "metadata": {
        "id": "CW8jycYLVI4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_callbacks(model_name):\n",
        "    return [\n",
        "        EarlyStopping(monitor='val_f1_score', patience=5, mode='max', verbose=1),\n",
        "        ModelCheckpoint(\n",
        "            f'best_{model_name}.h5',\n",
        "            monitor='val_f1_score',\n",
        "            save_best_only=True,\n",
        "            mode='max'\n",
        "        ),\n",
        "        LearningRateScheduler(lr_scheduler),\n",
        "        MetricSwitcher(switch_epoch=10)\n",
        "    ]"
      ],
      "metadata": {
        "id": "-n0eSyBqVF0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Warmup***"
      ],
      "metadata": {
        "id": "m3eztTIwVQEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WarmupCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, warmup_epochs=3):\n",
        "        super().__init__()\n",
        "        self.warmup_epochs = warmup_epochs\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        if epoch < self.warmup_epochs:\n",
        "            for layer in self.model.layers:\n",
        "                if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
        "                    layer.trainable = False\n",
        "        else:\n",
        "            for layer in self.model.layers:\n",
        "                layer.trainable = True"
      ],
      "metadata": {
        "id": "_KbuSsrOVMah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Очистка текста***"
      ],
      "metadata": {
        "id": "3KSKSNN6W_Bl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Технический код чтобы работал морф\n",
        "import inspect\n",
        "if not hasattr(inspect, 'getargspec'):\n",
        "    import collections\n",
        "    def getargspec(func):\n",
        "        sig = inspect.signature(func)\n",
        "        args = []\n",
        "        varargs = None\n",
        "        varkw = None\n",
        "        defaults = []\n",
        "        for param in sig.parameters.values():\n",
        "            if param.kind == param.VAR_POSITIONAL:\n",
        "                varargs = param.name\n",
        "            elif param.kind == param.VAR_KEYWORD:\n",
        "                varkw = param.name\n",
        "            else:\n",
        "                args.append(param.name)\n",
        "                if param.default is not param.empty:\n",
        "                    defaults.append(param.default)\n",
        "        return collections.namedtuple('ArgSpec', 'args varargs keywords defaults')(args, varargs, varkw, tuple(defaults) if defaults else None)\n",
        "    inspect.getargspec = getargspec"
      ],
      "metadata": {
        "id": "nWsk8Udwuikh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "morph = pymorphy2.MorphAnalyzer()\n",
        "stemmer = RussianStemmer()\n",
        "nltk.download('stopwords')\n",
        "russian_stopwords = set(stopwords.words('russian'))"
      ],
      "metadata": {
        "id": "qbggx99it-8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1) tf-idf + любая нейронная сеть**"
      ],
      "metadata": {
        "id": "43oFXAnEtldW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "nVXJcimm3ALy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text_tfidf(text):\n",
        "    # Удаление URL\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Удаление пунктуации\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Приведение к нижнему регистру\n",
        "    text = text.lower()\n",
        "    # Удаление чисел\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Токенизация\n",
        "    tokens = word_tokenize(text, language='russian')\n",
        "    # Лемматизация и удаление стоп-слов\n",
        "    tokens = [morph.parse(token)[0].normal_form for token in tokens if token not in russian_stopwords and len(token) > 2]\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "_vXC5SF41-zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2) w2v + любая нейронная сеть**"
      ],
      "metadata": {
        "id": "vZBdmVLCxJ1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text_w2v(text):\n",
        "    # Удаление URL\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Удаление пунктуации (кроме некоторых знаков)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation.replace('.', '').replace('!', '').replace('?', '')))\n",
        "    # Приведение к нижнему регистру\n",
        "    text = text.lower()\n",
        "    # Замена чисел на токен\n",
        "    text = re.sub(r'\\d+', '<NUM>', text)\n",
        "    # Токенизация\n",
        "    tokens = word_tokenize(text, language='russian')\n",
        "    # Удаление коротких токенов\n",
        "    tokens = [token for token in tokens if len(token) > 1]\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "p_HcNiGHxiuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3) встроенный эмбеддинг в нейросетевое решение на свёрточных сетях**"
      ],
      "metadata": {
        "id": "NY96600KxQkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text_cnn(text):\n",
        "    # Удаление URL (замена на токен)\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '<URL>', text, flags=re.MULTILINE)\n",
        "    # Удаление HTML тегов\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "    # Удаление пунктуации\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Приведение к нижнему регистру\n",
        "    text = text.lower()\n",
        "    # Замена эмодзи\n",
        "    text = re.sub(r'[\\U0001F600-\\U0001F64F]', '<EMOJI>', text)\n",
        "    # Замена чисел\n",
        "    text = re.sub(r'\\d+', '<NUM>', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "Kb107RvuxjHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4) встроенный эмбеддинг в нейросетевое решение на рекуррентных сетях**"
      ],
      "metadata": {
        "id": "mCn9T8GvxU3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text_rnn(text):\n",
        "    # Удаление URL (замена на токен)\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '<URL>', text, flags=re.MULTILINE)\n",
        "    # Удаление HTML тегов\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "    # Сохранение некоторых знаков препинания\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation.replace('.', '').replace('!', '').replace('?', '').replace(',', '')))\n",
        "    # Приведение к нижнему регистру\n",
        "    text = text.lower()\n",
        "    # Замена эмодзи\n",
        "    text = re.sub(r'[\\U0001F600-\\U0001F64F]', '<EMOJI>', text)\n",
        "    # Замена чисел\n",
        "    text = re.sub(r'\\d+', '<NUM>', text)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "mTq-zVTRxjb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5) эмбеддер + решение на bert-like моделях (любой вид)**"
      ],
      "metadata": {
        "id": "PjSUEuVAxdP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text_bert(text):\n",
        "    # Минимальная очистка для BERT\n",
        "    # Удаление лишних пробелов\n",
        "    text = ' '.join(text.split())\n",
        "    return text"
      ],
      "metadata": {
        "id": "FOAszhlsxjwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Применение очистки\n",
        "df_train['text_tfidf'] = df_train['text'].apply(clean_text_tfidf)\n",
        "df_train['text_w2v'] = df_train['text'].apply(clean_text_w2v)\n",
        "df_train['text_cnn'] = df_train['text'].apply(clean_text_cnn)\n",
        "df_train['text_rnn'] = df_train['text'].apply(clean_text_rnn)\n",
        "df_train['text_bert'] = df_train['text'].apply(clean_text_bert)"
      ],
      "metadata": {
        "id": "C7qBCNdQ2n0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_tfidf= test['text'].apply(clean_text_tfidf)\n",
        "X_test_w2v= test['text'].apply(clean_text_w2v)\n",
        "X_test_cnn= test['text'].apply(clean_text_cnn)\n",
        "X_test_rnn= test['text'].apply(clean_text_rnn)\n",
        "X_test_bert= test['text'].apply(clean_text_bert)\n",
        "test_ids = test['id']"
      ],
      "metadata": {
        "id": "KkzUVeQBHCRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_w2v"
      ],
      "metadata": {
        "id": "4cohLS4oK9R2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Поделить данные на трейн-валидацию**"
      ],
      "metadata": {
        "id": "1q0tt3lqZhri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Разделение на признаки и метки\n",
        "labels = df_train.filter(regex='label_').values\n",
        "texts_tfidf = df_train['text_tfidf'].values\n",
        "texts_w2v = df_train['text_w2v'].values\n",
        "texts_cnn = df_train['text_cnn'].values\n",
        "texts_rnn = df_train['text_rnn'].values\n",
        "texts_bert = df_train['text_bert'].values\n",
        "\n",
        "# Разделение на train/validation\n",
        "(X_train_tfidf, X_val_tfidf, y_train, y_val) = train_test_split(texts_tfidf, labels, test_size=0.2, random_state=42)\n",
        "(X_train_w2v, X_val_w2v, _, _) = train_test_split(texts_w2v, labels, test_size=0.2, random_state=42)\n",
        "(X_train_cnn, X_val_cnn, _, _) = train_test_split(texts_cnn, labels, test_size=0.2, random_state=42)\n",
        "(X_train_rnn, X_val_rnn, _, _) = train_test_split(texts_rnn, labels, test_size=0.2, random_state=42)\n",
        "(X_train_bert, X_val_bert, _, _) = train_test_split(texts_bert, labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Gnfg7QKx4V0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_w2v = df_train['text_w2v'].values\n",
        "texts_w2v"
      ],
      "metadata": {
        "id": "GrwLzKyzNfV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. tf-idf + любая нейронная сеть (скор на кагле)"
      ],
      "metadata": {
        "id": "CRQxl8B5KFvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = tfidf.fit_transform(X_train_tfidf).toarray()\n",
        "X_val_tfidf = tfidf.transform(X_val_tfidf).toarray()"
      ],
      "metadata": {
        "id": "jvMlQzyn5sve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " model = Sequential([\n",
        "     Dense(512, activation='relu', input_shape=(X_train_tfidf.shape[1],)),\n",
        "     Dropout(0.5),\n",
        "     Dense(256, activation='relu'),\n",
        "     Dropout(0.5),\n",
        "     Dense(y_train.shape[1], activation='sigmoid')\n",
        " ])\n",
        "\n",
        "\n",
        " model.compile(\n",
        "     optimizer=Adam(learning_rate=0.001),\n",
        "     loss='binary_crossentropy',\n",
        "     metrics=['accuracy']\n",
        " )\n",
        "\n",
        " history = model.fit(\n",
        "     X_train_tfidf, y_train,\n",
        "     validation_data=(X_val_tfidf, y_val),\n",
        "     epochs=10,\n",
        "     batch_size=32,\n",
        "     verbose=1\n",
        " )\n",
        "\n",
        "\n",
        " y_pred = model.predict(X_val_tfidf)\n",
        " y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "\n",
        " print(classification_report(y_val, y_pred_binary, target_names=[f'label_{i}' for i in range(y_train.shape[1])]))"
      ],
      "metadata": {
        "id": "o5Rs-9S06Eyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_tfidf = tfidf.transform(X_test_tfidf).toarray()\n",
        "\n",
        "\n",
        "y_test_pred = model.predict(X_test_tfidf)\n",
        "y_test_pred_binary = (y_test_pred > 0.5).astype(int)\n",
        "\n",
        "\n",
        "labels = [' '.join(map(str, row)) for row in y_test_pred_binary]\n",
        "\n",
        "\n",
        "results = pd.DataFrame({\n",
        "     'id': test_ids,\n",
        "     'labels': labels\n",
        " }).sort_values('id')"
      ],
      "metadata": {
        "id": "KOOejWmbHsn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ss = pd.read_csv('/content/drive/MyDrive/DL/comp2/sample_submission.csv')\n",
        "labels_as_string =[]\n",
        "for row in y_test_pred_binary:\n",
        "  str_row = [str(label) for label in row]\n",
        "  labels_as_string.append(' '.join(str_row))\n",
        "ss['labels'] = labels_as_string\n",
        "ss.to_csv('sample_submission.csv', index = False)"
      ],
      "metadata": {
        "id": "COmRHWjyH24m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ss"
      ],
      "metadata": {
        "id": "iMtpAFkHICAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  w2v + любая нейронная сеть"
      ],
      "metadata": {
        "id": "hgc3frUZGThK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, Flatten, Conv1D, GlobalMaxPooling1D,LSTM"
      ],
      "metadata": {
        "id": "sL4D78aXIuHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Обучение Word2Vec модели\n",
        "tokenized_texts = [text.split() for text in X_train]\n",
        "w2v_model = Word2Vec(tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Создание матрицы эмбеддингов\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in w2v_model.wv:\n",
        "        embedding_matrix[i] = w2v_model.wv[word]\n",
        "\n",
        "# Преобразование текстов в последовательности\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train_w2v)\n",
        "X_val_seq = tokenizer.texts_to_sequences(X_val_w2v)\n",
        "X_test_seq = tokenizer.texts_to_sequences(test['text'])\n",
        "\n",
        "max_len = 100\n",
        "X_train_w2v = pad_sequences(X_train_seq, maxlen=max_len)\n",
        "X_val_w2v = pad_sequences(X_val_seq, maxlen=max_len)\n",
        "X_test_w2v = pad_sequences(X_test_seq, maxlen=max_len)\n",
        "\n",
        "# Модель\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_len, trainable=False),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(y_train.shape[1], activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Обучение\n",
        "history = model.fit(\n",
        "    X_train_w2v, y_train,\n",
        "    validation_data=(X_val_w2v, y_val),\n",
        "    epochs=10,\n",
        "    batch_size=128,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Оценка\n",
        "y_pred = model.predict(X_val_w2v)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "print(\"Word2Vec + NN Classification Report:\")\n",
        "print(classification_report(y_val, y_pred_binary, target_names=[f'label_{i}' for i in range(y_train.shape[1])]))"
      ],
      "metadata": {
        "id": "X6RBd38RGWP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_pred = model.predict(X_test_w2v)\n",
        "y_test_pred_binary = (y_test_pred > 0.5).astype(int)\n",
        "\n",
        "\n",
        "labels = [' '.join(map(str, row)) for row in y_test_pred_binary]\n",
        "\n",
        "\n",
        "results = pd.DataFrame({\n",
        "     'id': test_ids,\n",
        "     'labels': labels\n",
        " }).sort_values('id')"
      ],
      "metadata": {
        "id": "4Bh_F2m8L0RY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ss = pd.read_csv('/content/drive/MyDrive/DL/comp2/sample_submission.csv')\n",
        "labels_as_string =[]\n",
        "for row in y_test_pred_binary:\n",
        "  str_row = [str(label) for label in row]\n",
        "  labels_as_string.append(' '.join(str_row))\n",
        "ss['labels'] = labels_as_string\n",
        "ss.to_csv('sample_submission.csv', index = False)"
      ],
      "metadata": {
        "id": "Vebya9TMMJo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3."
      ],
      "metadata": {
        "id": "vGqnog6nSVlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "max_len = 100\n",
        "\n",
        "# 2. Преобразование текстов в последовательности\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
        "X_test_seq = tokenizer.texts_to_sequences(test['text'])\n",
        "\n",
        "# 3. Добавление паддинга\n",
        "X_train_cnn = pad_sequences(X_train_seq, maxlen=max_len)\n",
        "X_val_cnn = pad_sequences(X_val_seq, maxlen=max_len)\n",
        "X_test_cnn = pad_sequences(X_test_seq, maxlen=max_len)\n",
        "\n",
        "# 4. Создание модели CNN\n",
        "model = Sequential([\n",
        "    Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=100,\n",
        "        input_length=max_len\n",
        "    ),\n",
        "    Conv1D(128, 5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(y_train.shape[1], activation='sigmoid')  # y_train - one-hot encoded labels\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 5. Обучение модели\n",
        "history = model.fit(\n",
        "    X_train_cnn, y_train,\n",
        "    validation_data=(X_val_cnn, y_val),\n",
        "    epochs=10,\n",
        "    batch_size=128,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 6. Оценка модели\n",
        "y_pred = model.predict(X_val_cnn)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "print(\"CNN Classification Report:\")\n",
        "print(classification_report(y_val, y_pred_binary, target_names=[f'label_{i}' for i in range(y_train.shape[1])]))"
      ],
      "metadata": {
        "id": "yW773PSySUxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4."
      ],
      "metadata": {
        "id": "W9rvopXs1Xhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_len, trainable=False),\n",
        "    LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(y_train.shape[1], activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_w2v, y_train,\n",
        "    validation_data=(X_val_w2v, y_val),\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "y_pred = model.predict(X_val_w2v)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "print(\"RNN (LSTM) Classification Report:\")\n",
        "print(classification_report(y_val, y_pred_binary, target_names=[f'label_{i}' for i in range(y_train.shape[1])]))"
      ],
      "metadata": {
        "id": "2NzJy31K166s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5."
      ],
      "metadata": {
        "id": "s3okhL_h1Oc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, TFBertModel\n",
        "import tensorflow as tf\n",
        "\n",
        "# Загрузка BERT модели и токенизатора\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "# Токенизация\n",
        "X_train_bert = tokenizer(X_train.tolist(), padding=True, truncation=True, max_length=100, return_tensors=\"tf\")\n",
        "X_val_bert = tokenizer(X_val.tolist(), padding=True, truncation=True, max_length=100, return_tensors=\"tf\")\n",
        "X_test_bert = tokenizer(X_test.tolist(), padding=True, truncation=True, max_length=100, return_tensors=\"tf\")\n",
        "\n",
        "# Создание модели\n",
        "input_ids = tf.keras.layers.Input(shape=(100,), dtype=tf.int32, name=\"input_ids\")\n",
        "attention_mask = tf.keras.layers.Input(shape=(100,), dtype=tf.int32, name=\"attention_mask\")\n",
        "\n",
        "bert_output = bert_model(input_ids, attention_mask=attention_mask)[1]\n",
        "dense1 = tf.keras.layers.Dense(256, activation='relu')(bert_output)\n",
        "dropout = tf.keras.layers.Dropout(0.5)(dense1)\n",
        "output = tf.keras.layers.Dense(y_train.shape[1], activation='sigmoid')(dropout)\n",
        "\n",
        "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=2e-5),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Обучение\n",
        "history = model.fit(\n",
        "    {'input_ids': X_train_bert['input_ids'], 'attention_mask': X_train_bert['attention_mask']},\n",
        "    y_train,\n",
        "    validation_data=({'input_ids': X_val_bert['input_ids'], 'attention_mask': X_val_bert['attention_mask']}, y_val),\n",
        "    epochs=3,  # BERT требует меньше эпох\n",
        "    batch_size=16,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Оценка\n",
        "y_pred = model.predict({'input_ids': X_val_bert['input_ids'], 'attention_mask': X_val_bert['attention_mask']})\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "print(\"BERT Classification Report:\")\n",
        "print(classification_report(y_val, y_pred_binary, target_names=[f'label_{i}' for i in range(y_train.shape[1])]))"
      ],
      "metadata": {
        "id": "ygEWWohW19FS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_test_clean = test['text'].apply(basic_clean)\n",
        "# test_ids = test['id']\n",
        "\n",
        "# X_test_tfidf = tfidf.transform(X_test_clean).toarray()\n",
        "\n",
        "\n",
        "# y_test_pred = model.predict(X_test_tfidf)\n",
        "# y_test_pred_binary = (y_test_pred > 0.5).astype(int)\n",
        "\n",
        "\n",
        "# labels = [' '.join(map(str, row)) for row in y_test_pred_binary]\n",
        "\n",
        "\n",
        "# results = pd.DataFrame({\n",
        "#     'id': test_ids,\n",
        "#     'labels': labels\n",
        "# }).sort_values('id')"
      ],
      "metadata": {
        "id": "-tt3Hi0UuawY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_test_clean = test['text'].apply(basic_clean)\n",
        "# test_ids = test['id']\n",
        "\n",
        "# X_test_tfidf = tfidf.transform(X_test_clean).toarray()\n",
        "\n",
        "\n",
        "# y_test_pred = model.predict(X_test_tfidf)\n",
        "# y_test_pred_binary = (y_test_pred > 0.5).astype(int)\n",
        "\n",
        "\n",
        "# labels = [' '.join(map(str, row)) for row in y_test_pred_binary]\n",
        "\n",
        "\n",
        "# results = pd.DataFrame({\n",
        "#     'id': test_ids,\n",
        "#     'labels': labels\n",
        "# }).sort_values('id')\n"
      ],
      "metadata": {
        "id": "rIxEzT2A6Jp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3x7dWhppEZTv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}